{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501e6bee-a9b9-4326-a0c3-69fc2d6586df",
   "metadata": {},
   "source": [
    "# **Import Libraries**\n",
    "\n",
    "Throughtout this project we will use:\n",
    "\n",
    "* **OpenCV**\n",
    "* **Dlib**\n",
    "* **NumPy**\n",
    "* **Pillow** (will be used to help display images here in Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03c70da-6d4f-4af9-9ce7-074cded39e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import glob\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110e547-8de2-4749-90dd-b3f29e53b6e8",
   "metadata": {},
   "source": [
    "# **Exctract facial features**<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup.jpg\" width=400 height=300>\n",
    "</div> <br>\n",
    "\n",
    "With the help of **dlib**, we will extract the facial features of each image, as demonstraded in the picture above. We will use **get_frontal_face_detector** to detect where the face is, and **shape_predictor_68_face_landmarks.dat** to extract the 68 landmarks of the face:<br>\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/1027187677403029575/1332853023982092328/Screenshot_from_2025-01-25_23-09-41.png?ex=6796c39c&is=6795721c&hm=f73d98a5358d6dc37334a35abbb19fa9560d9d85cc26a8313599d659f5404ad2&\" alt=\"Image 1\" style=\"width: 50%; margin-right: 10px;\" />\n",
    "    <img src=\"https://cdn.discordapp.com/attachments/1027187677403029575/1332853066096971806/Screenshot_from_2025-01-25_23-10-20.png?ex=6796c3a6&is=67957226&hm=9fc02936c120d620f4c82fe383a3e2b1cf22d55e8bd7dcc0332be1dc84ec1d69&\" alt=\"Image 3\" style=\"width: 50%;\" />\n",
    "</div><br>\n",
    "\n",
    "# **Normalize images**<br>\n",
    "\n",
    "In drawing, faces can be simplified using the “Rule of Thirds,” which divides the face into horizontal and vertical thirds to help position facial features accurately.<br><br>\n",
    "\n",
    "**Rule of Thirds:** The face can be divided into three equal horizontal sections. The top third is from the hairline to the eyebrows, the middle third is from the eyebrows to the bottom of the nose, and the bottom third is from the bottom of the nose to the chin. This method helps ensure that facial features are proportionally placed.<br>\n",
    "**Facial Features Placement:** Eyes are typically positioned halfway between the top of the head and the chin, which aligns with the horizontal third division. The nose line is found in the middle of the eye line and the bottom of the chin, and the mouth line is about one-third of the way down from the nose line to the chin.<br><br>\n",
    "\n",
    "Since each image has variant size, we need to normalize them, warping each to a **600x600 image**. With the drawing rules stated earlier, let's define where certain facial features will warp to:<br><br>\n",
    "\n",
    "* Left corner of the **left eye**: (180, 200)\n",
    "* Right corner of the **right eye**: (420, 200)\n",
    "* **Bottom lip**: (300, 400)\n",
    "\n",
    "Now that we know the starting and ending positions, we can use the similarity transform (rotation, translation and scale). To find this transformation, we will use **getAffineTransform**. With this matrix, we can warp the image with **warpAffine** and update the landmarks:<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://learnopencv.com/wp-content/ql-cache/quicklatex.com-b6e614b5448854f2c83abcb6e5786774_l3.png\" width=400 height=300>\n",
    "</div> <br>\n",
    "\n",
    "The *first* and *second* columns **rotate and scale** the vector. You will need you to add the *last* column, that represents the **translation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b521567-aad7-49c2-a84d-e38e12b99cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "IMG_DIM = 600\n",
    "LEFT_EYE_CORNER = 36\n",
    "RIGHT_EYE_CORNER = 45\n",
    "BOTTOM_LIP = 57\n",
    "\n",
    "\n",
    "def detectFace(img):\n",
    "    \"\"\"\n",
    "    Detect the face present in the image \n",
    "    \n",
    "    Args:\n",
    "      img (ndarray (width, height, 3)): Image\n",
    "      \n",
    "    Returns\n",
    "        face (dlib_rectangles): Rectangle that delimits the face\n",
    "    \"\"\"    \n",
    "    face = detector(img, 1)\n",
    "    # if len(face) > 1 return largest\n",
    "    # Couldn't detect any faces in the image\n",
    "    if len(face) == 0:\n",
    "        return None\n",
    "    \n",
    "    return face[0]\n",
    "\n",
    "\n",
    "def normalizeImage(img, landmarks):\n",
    "    \"\"\"\n",
    "    Warps the image input to a 600x600 image with the aid of the outer corner of the eyes and the bottom lip \n",
    "    \n",
    "    Args:\n",
    "      img (ndarray (width, height, 3)): Image\n",
    "      landmarks (dlib_full_object_detection (68)): Facial landmarks of the image\n",
    "      \n",
    "    Returns\n",
    "        (normalized_img, updated_landmarks) (ndarray (600, 600, 3), ndarray (68, 2)): Warped 600x600 image\n",
    "                                                                                        and landmarks for the warped image\n",
    "    \"\"\"    \n",
    "    left_eye = landmarks.part(LEFT_EYE_CORNER)\n",
    "    right_eye = landmarks.part(RIGHT_EYE_CORNER)\n",
    "    bot_lip = landmarks.part(BOTTOM_LIP)\n",
    "\n",
    "    # Create source array with float32 type\n",
    "    src = np.array([[left_eye.x, left_eye.y],\n",
    "                    [right_eye.x, right_eye.y],\n",
    "                    [bot_lip.x, bot_lip.y]], dtype=np.float32)\n",
    "    \n",
    "    # Create destination array with float32 type\n",
    "    dst = np.array([[180, 200],\n",
    "                    [420, 200],\n",
    "                    [300, 400]], dtype=np.float32)\n",
    "\n",
    "    # Get transformation matrix\n",
    "    T = cv2.getAffineTransform(src, dst)\n",
    "\n",
    "    # Warp Image\n",
    "    normalized_img = cv2.warpAffine(img, T, (IMG_DIM,IMG_DIM))\n",
    "\n",
    "    # Update landmarks (T*landmarks + translation), but first convert to ndarray\n",
    "    npLandmarks = np.array([[landmarks.part(i).x, landmarks.part(i).y] for i in range(68)], dtype=np.float32)\n",
    "    last_row = T[:, -1]\n",
    "    translation = np.array([last_row for _ in range(68)], dtype=np.float32)\n",
    "    \n",
    "    updated_landmarks = (np.delete(T, -1, axis=1) @ npLandmarks.transpose()).transpose() + translation\n",
    "\n",
    "    return (normalized_img, updated_landmarks.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c1140a-64d9-48f4-8ad2-62b97737d434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: images/Donald_Trump.webp\n",
      "Loading: images/Elon_Musk.webp\n",
      "Loading: images/Mark_Zuckerberg.webp\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "images_path = glob.glob(\"images/*\")\n",
    "normalized_imgs = []\n",
    "\n",
    "for img_path in images_path:\n",
    "    img = cv2.imread(img_path)\n",
    "    face = detectFace(img)\n",
    "\n",
    "    if face is None:\n",
    "        print(\"Error: Couldn't detect any faces in:\", img_path)\n",
    "    else:\n",
    "        print(\"Loading:\", img_path)\n",
    "\n",
    "        landmarks = predictor(img, face)\n",
    "        normalized_img, landmarks = normalizeImage(img, landmarks)\n",
    "\n",
    "        # Add more points to landmarks to help with alignment later on\n",
    "        pass\n",
    "\n",
    "        normalized_imgs.append((normalized_img, landmarks))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "876b5e08-76ab-4b04-bc2a-319ccf680ab1",
   "metadata": {},
   "source": [
    "# Run this code to display the normalized images with facial features\n",
    "# Note: OpenCV uses BGR colorscheme\n",
    "\n",
    "RED = (0, 0, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "\n",
    "for img, landmarks in normalized_imgs:\n",
    "    # Draw a circle for each landmark\n",
    "    for x, y in landmarks:\n",
    "        cv2.circle(img, (x, y), 2, GREEN, -1)\n",
    "\n",
    "    # Convert to RGB, then to PIL\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(rgb_img)\n",
    "\n",
    "    # You can also use pil_img.show()\n",
    "    display(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b216b3ba-df2f-4eae-9140-aae96a0196ca",
   "metadata": {},
   "source": [
    "# **Align faces**\n",
    "\n",
    "Now that the images are normalized, we need to allign the rest of the remaining features. To do that, we will use **Delaunay Triangulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7d70d-d1dc-4859-9677-f66d6d4d6cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:average_face]",
   "language": "python",
   "name": "conda-env-average_face-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
